---
title: "Exploratory Look at Medical Jurisprudence Documents"
output: html_notebook
---

Vectorize a subcorpus.
```{r}
library(tokenizers)
library(text2vec)
library(readr)
#library(doParallel)
library(Matrix)
library(broom)
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)
library(lubridate)



#load the metadata file that I will need later
us_items <- readRDS("us_items.rds")

#Sorry, no parallel universe for Windows users
#N_WORKERS <- 8
#registerDoParallel(N_WORKERS)
```
Now, read in the files, tokenize, create a vocabulary and prune it, and then vectorize it.  Lastly, create a document term matrix (dtm.)
```{r}
files <- list.files("C:/Users/Kim/Documents/Text Analysis for Historians/medical_documents", 
                    pattern = "*.txt",
                    full.names = TRUE)

reader <- function(f) {
  require(stringr)
  n <- basename(f) %>% str_replace("\\.txt", "")
  doc <- readr::read_file(f)
  names(doc) <- n
  doc
}

jobs <- files %>% 
  #split_into(N_WORKERS) %>% NO SOUP FOR YOU
  lapply(ifiles, reader = reader) %>% 
  lapply(itoken, chunks_number = 1, tokenizer = tokenizers::tokenize_words,
         progressbar = FALSE)

vocab <- create_vocabulary(jobs)
pruned <- prune_vocabulary(vocab, term_count_min = 10,
                           term_count_max = 50e3)
message("Keeping ", round(nrow(pruned$vocab) / nrow(vocab$vocab), 3) * 100,
        "% of the vocabulary.")
vectorizer <- vocab_vectorizer(pruned)

dtm <- create_dtm(jobs, vectorizer)
```
Count the vocabulary.
```{r}

rowSums(dtm) %>% tidy() %>% View

dtm[ , c("medical", "insanity", "child")] %>% tidy() %>% View


# %>% 
#   tidy() %>% 
#   arrange(desc(x))
```
Make a chart of certain words.

```{r}

dtm_to_df <- function(x, words) {
  require(dplyr)
  require(tibble)
  require(stringr)
  require(Matrix)
  stopifnot(is.character(words))
  out <- as_tibble(as.data.frame(as.matrix(x[, words])))
  colnames(out) <- words
  ids <- str_replace_all(rownames(x), "\\.txt", "")
  ids <- str_split_fixed(ids, "-", n = 2)
  out %>% 
    mutate(document_id = ids[ , 1, drop = TRUE],
           page_id = ids[ , 2, drop = TRUE]) %>% 
    select(document_id, page_id, everything())
}

words_of_interest <- c("man", "woman", "male", "female", "child")

counts <- dtm_to_df(dtm, words_of_interest) %>% 
  gather(word, count, -document_id, -page_id) %>% 
  filter(count > 0)

item_years <- us_items %>% 
  select(document_id, publication_date)
#and then convert those dates to just the year
item_years$publication_date<-year(as.Date(item_years$publication_date, origin = '1900-1-1'))
item_years$document_id<-as.character(item_years$document_id)

# Still needs to be normalized
counts %>% 
  group_by(document_id, word) %>% 
  summarize(count = sum(count)) %>% 
  left_join(item_years, by = "document_id") %>% 
  group_by(publication_date, word) %>% 
  summarize(count = sum(count)) %>% 
  ggplot(aes(x = publication_date, y = count, color = word)) +
  geom_point() +
  geom_smooth(span = 0.1, se = FALSE) +
  labs(title = "Word use over time in medical jurisprudence treatises")# + 
  xlim
```
Distance functions
```{r}
distances <- dist2(dtm[1, , drop = FALSE], dtm[1:1e2, ])
distances2 <- distances[1, ] %>% sort()
head(distances2)
tail(distances2)
range(distances2)

similarities <- wordVectors::cosineSimilarity(dtm[1:123, , drop = FALSE], 
                                              dtm[1:123, , drop = FALSE])
similarities 
```
TF-IDF
```{r}
model_tfidf <-  TfIdf$new()
model_tfidf$fit(dtm)
dtm_tfidf <- model_tfidf$transform(dtm)

dtm_1 = model_tfidf$transform(dtm)
dtm_2 = model_tfidf$fit_transform(dtm)
identical(dtm_1, dtm_2)

#this has normalized individual words so that we can see which words actualy have more significance (medical vs poison)  Go reread leskovic to see the intuition behind

rowSums(dtm_1) %>% tidy() %>% View
```
